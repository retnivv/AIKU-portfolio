# ğŸ“˜ CS231n Assignment 2  

ì´ í´ë”ëŠ” Stanford CS231n ê³¼ì œ 2ì˜ ì‹¤ìŠµ ê¸°ë¡ê³¼ êµ¬í˜„ ë‚´ìš©ì„ ì •ë¦¬í•œ ê³µê°„ì…ë‹ˆë‹¤.  
ê³¼ì œ 2ëŠ” Multi-Layer Fully Connected Network, Batch normalization, Dropout, CNNì„ ì§ì ‘ êµ¬í˜„í•˜ê³ ,  
PyTorchë¥¼ í™œìš©í•œ ëª¨ë¸ í•™ìŠµ ì‹¤ìŠµ ê³¼ì •ì„ í¬í•¨í•©ë‹ˆë‹¤.

---

## ğŸ“ í´ë” êµ¬ì¡°

<pre><code>
assignment2/
â”œâ”€â”€ README.md                        # í˜„ì¬ ë¬¸ì„œ
â”œâ”€â”€ BatchNormalization.ipynb        # Batch Normalization ì‹¤ìŠµ ë…¸íŠ¸ë¶
â”œâ”€â”€ Dropout.ipynb                   # Dropout ì‹¤ìŠµ ë…¸íŠ¸ë¶
â”œâ”€â”€ ConvolutionalNetworks.ipynb     # CNN ì‹¤ìŠµ ë…¸íŠ¸ë¶
â”œâ”€â”€ ../py/                          # classifier ë° layer êµ¬í˜„ ì½”ë“œ
â”‚   â”œâ”€â”€ fc_net.py                   # Fully Connected Network êµ¬í˜„
â”‚   â”œâ”€â”€ layers.py                   # Affine, Batchnorm, Dropout ë“± ê°ì¢… Layer êµ¬í˜„
â”‚   â”œâ”€â”€ cnn.py                      # Three Layer CNN êµ¬í˜„
â”‚   â”œâ”€â”€ optim.py                    # SGD, Adam ë“± Optimizer êµ¬í˜„
â”‚   â”œâ”€â”€ layer_utils.py              # Affine+Relu ë“± ê°ì¢… í¸ì˜ì„± Layer ëª¨ë“ˆ
â”‚   â”œâ”€â”€ fast_layers.py              # ë¹ ë¥¸ ë²„ì „ì˜ Convolutional Layer ëª¨ë“ˆ
â”‚   â””â”€â”€ solver.py                   # ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤
â”œâ”€â”€ ../figures/                     # ê´€ë ¨ ì´ë¯¸ì§€
</code></pre>

â€» `py/` í´ë”ì—ëŠ” í•µì‹¬ êµ¬í˜„ ì½”ë“œ('.py' íŒŒì¼)ë“¤ì´ ë“¤ì–´ìˆìœ¼ë©°, `.ipynb` íŒŒì¼ì—ì„œ ì´ë¥¼ importí•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.
â€» `figures/` í´ë” ë‚´ì˜ ì´ë¯¸ì§€ëŠ” ì‹¤ìŠµ ê³¼ì • ì¤‘ í•„ìš”í•œ ê³„ì‚°ì„ ì§ì ‘ ìˆ˜í–‰í•˜ê³  ì •ë¦¬í•œ ìë£Œì…ë‹ˆë‹¤.

---

## ğŸ“„ ê³¼ì œ ê°œìš”

### ğŸŸ¦ `BatchNormalization.ipynb` - Batch Normalization

- **Batch Normalization ë° Layer Normalization êµ¬í˜„** (`layers.py`)
- **ì‹¤ìŠµ ì¤‘ ìˆ˜ê¸°ë¡œ ì •ë¦¬í•œ ì—­ì „íŒŒ ê³„ì‚° ê·¸ë˜í”„:**

<details> <summary><strong>ğŸ“Œ BN backward pass</strong></summary> <p align="center"> 
<img src="https://github.com/retnivv/AIKU-portfolio/raw/main/cs231n/assignment2/image/batchnorm_backward.jpg" width="750"/> </p> </details> <details> <summary><strong>ğŸ“Œ BN alternative backward pass</strong></summary> <p align="center"> 
<img src="https://github.com/retnivv/AIKU-portfolio/raw/main/cs231n/assignment2/image/batchnorm_backward_alt.jpg" width="750"/> </p> </details>

---

### ğŸŸ¨ `Dropout.ipynb` - Dropout

- **Dropout ê³„ì¸µì˜ forward & backward êµ¬í˜„** (`layers.py`)
- **Dropoutì„ ì ìš©í–ˆì„ ë•Œì™€ ì ìš©í•˜ì§€ ì•Šì•˜ì„ ë•Œ small datasetìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ì˜ ì„±ëŠ¥ ë¹„êµ**

---

### ğŸŸ¥ `ConvolutionalNetworks.ipynb` - CNN

- **Convolutional layer forward & backward êµ¬í˜„** (`layers.py`)
- **Three Layer CNN êµ¬í˜„** (`cnn.py`)
- **(+) Spatial Batch Normalization, Spatial Group Normalization êµ¬í˜„** (`layers.py`)

---

## ğŸ§  í•™ìŠµ/ì‹¤í—˜ ì¤‘ ê¹¨ë‹¬ì€ ì 

- BatchNormalization.ipynb
  - BatchNormì˜ ì—­ì „íŒŒ ê³„ì‚°ê³¼ì •
  - BatchNormê³¼ LayerNormì˜ ë¹„êµ
- Dropout.ipynb
  - Dropoutì˜ ì •ê·œí™” ê¸°ëŠ¥
- ConvolutionalNetworks.ipynb
  - Convolutional layerì˜ (naive) forward, backward ê³„ì‚° ê³¼ì •
  - Spatial Batch Normalization, Spatial Group Normalizationê³¼ BatchNorm, LayerNormì˜ ë¹„êµ
---

## âœï¸ ê¸°íƒ€ ì •ë³´

- ì‹¤ìŠµ í™˜ê²½: Google Colab  
- CIFAR-10 ë°ì´í„°ì…‹ ì‚¬ìš© 

---

## ğŸ“ ì°¸ê³ 

> ë³¸ ë…¸íŠ¸ë¶ì€ í•™ìŠµ ëª©ì ì˜ ì‹¤ìŠµ ê²°ê³¼ë¡œ, ëª¨ë“  êµ¬í˜„ì€ ì§ì ‘ ì‘ì„± ë° í…ŒìŠ¤íŠ¸í•˜ì˜€ìŠµë‹ˆë‹¤.  
> `.ipynb` íŒŒì¼ì€ ì£¼ì„ ë° ë§ˆí¬ë‹¤ìš´ ì…€ì˜ ì •ì œë¥¼ ìµœì†Œí™”í•˜ê³ , ì‹¤ìŠµ ë‹¹ì‹œì˜ íë¦„ê³¼ ê³ ë¯¼ì´ ìì—°ìŠ¤ëŸ½ê²Œ ë“œëŸ¬ë‚˜ë„ë¡ ì›ë³¸ í˜•íƒœë¥¼ ìµœëŒ€í•œ ìœ ì§€í•˜ì˜€ìŠµë‹ˆë‹¤.
> `figures/` í´ë” ë‚´ ì´ë¯¸ì§€ ë˜í•œ ì›ë³¸ ê³„ì‚° íë¦„ì„ ë³´ì¡´í•˜ê³ ì ê·¸ëŒ€ë¡œ ì²¨ë¶€í•˜ì˜€ìŠµë‹ˆë‹¤.
