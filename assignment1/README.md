# ğŸ“˜ CS231n Assignment 1

This folder contains my implementation and experimentation for **Assignment 1 of Stanford CS231n**.

The assignment focuses on implementing **Linear SVM**, **Softmax classifier**, and a **Two-Layer Net**,  
and improving their performance through hyperparameter tuning on the CIFAR-10 dataset.

---

## ğŸ“ Directory Structure

<pre><code>
assignment1/
â”œâ”€â”€ svm.ipynb                 # SVM classifier experiment notebook
â”œâ”€â”€ softmax.ipynb             # Softmax classifier experiment notebook
â”œâ”€â”€ two_layer_net.ipynb       # Two-layer neural network experiment
â”œâ”€â”€ README.md                 
â””â”€â”€ ../py/                    # Core implementation files
    â”œâ”€â”€ linear_svm.py         # SVM loss and gradient
    â”œâ”€â”€ softmax.py            # Softmax loss and gradient
    â”œâ”€â”€ fc_net.py             # Two-layer network implementation
    â”œâ”€â”€ layer.py              # Affine, ReLU, and related layers
    â”œâ”€â”€ layer_utils.py        # Utility layers (Affine + ReLU)
    â”œâ”€â”€ optim.py              # Optimizers (SGD, Adam)
    â”œâ”€â”€ solver.py             # Training loop abstraction
    â””â”€â”€ linear_classifier.py  # Shared classifier logic
</code></pre>

**Note:** The `py/` directory contains the core implementation files (`.py`), which are imported and used within the corresponding Jupyter notebooks (`.ipynb`).


---

## ğŸ“„ Assignment Overview

### ğŸŸ¦ `svm.ipynb` - Linear SVM Classifier

- SVM loss and gradient computation implemented using both naive loops and vectorized operations (`linear_svm.py`)

---

### ğŸŸ¨ `softmax.ipynb` - Softmax Classifier

- Softmax loss and gradient computation implemented using both naive loops and vectorized operations (`softmax.py`)

---

### ğŸŸ¥ `two_layer_net.ipynb` - Two-Layer Net

- Implementation of a two-layer neural network and exploration of hyperparameter tuning
